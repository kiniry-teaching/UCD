Revision:
0.1		ED	First draft

Title:		Kinephon
Team:		Intuitive Aptitudes
Members:	Eva Darulova	05817625
			Sumbo Ajisafe	05005451
			David Swords	05477689
			Eliott Bartley	05806500
			Jakub Dostal	05844151

PROJECT DESIGN

Installer
	This installer could come in different flavours, depending on how the project comes along.
	The two main options are one for Linux and one for Windows.
	Linux:
		This will be a .deb (Debian) package. Itself can come in two flavours which are binary and
		source. A binary package contains all the executables, man pages etc. and are unpacked using
		the Debian utility dpkg. A source package contains a .dsc file describing the source package itself,
		a orig.tar.gz containing the original source code and a diff.gz describing the Debian specific changes
		to the source. The dpkg-source, packs and unpacks the Debian source archives.
	Windows:
		For this it will be likely that we will produce the MSI files with a Third-party app.
		Though we could also use Visual Studio 8 to produce an installer also.

Configuration
	Before the program can be fully started, we need to establish a connection with the Wiimote and check
	that the Wiimote sees the appropriate number of points (if we decide to require at least a given number
	of points). Also, it will probably be necessary to configure the system itself e.g. for specific types
	of music that we want to produce. After all of these things are configured, the program will be run. 

Interface
	<dave, jakub>
	#TO DO - We can chat about the nuts and bolts of this in the meeting tomorrow. Signed Dave
	- finger band and/or wrist band with 1+ LEDs to make sure that the point is always visible.
	- static Wiimote near the display with line of sight visibility.

    # Can a schematic be included with this of the electronics

Camera
	The IR camera in the Wiimote has a resolution of 1024x768. In front of the camera, there is a special hardware filter,
	which tracks up to four IR blobs. Their locations are recorded into memory. This data can be accessed
	through bluetooth HCI interface by querying the Wiimote. The reports can be sent up to 100 times per 
	second, which is a hardware limit caused by the camera and the tracking filter.  

Connection to Computer
	In order to extract the necessary data (in form of reports) from the Wiimote, we will need to establish
	a stable connection with the Wiimote through the available bluetooth HCI interface. After the connection
	is successfully established, we will query the Wiimote on the appropriate port(s) to get the reports. 
	If necessary, we will write specific instructions to the Wiimote memory over the same interface to
	specify the format of the reports further. The reports that will be received from the Wiimote will consist
	of a channel ID/report ID and several bytes of payload. The reports will then be passed to the parser using 
	an API between the two. 

Parser
	<dave>
	Just to get this started, an interface I could make use of would be just a single 'get' method that
	 would return the next piece of information in a class or struct containing { id, x, y, size }. I suppose
	 if it were to be extended, I could ask how many blobs are you tracked and 'get' by a blob id.
	An alternative would be to use a callback event. I will call you once and pass the address of a callback
	 function. Every time you get new data, you call my callback and pass the { id, x, y, size } struct, or
	 (it's such a small data-structure) you could just pass it as parameters. This option would be useful
	 if you need to constantly watch the input stream so there are no timeouts, and are going to thread the
	 reading of the data

Recorder and Analyser[EB]
	During initialisation Kinephon will call on the ShapeLoader, ShapeLoader will open a file containing all
	the different gestures and load them in returning a Shapes collection. A gesture is a physical movement
	made by the user, recorded by the WiiMotem and stored as an array of vectors in 2 dimensional space (a
	gesture actually has a more general meaning, but this is described below). A Shapes collection has two
	functions, hold an array of Shape objects and compare a single gesture against each shape to find near
	matches. A Shape is made up a 2 dimensional array of 'weights' and an array of Zone objects. The 2
	dimensional array of 'weights' describe a grid over the most likely areas which a gesture will travel to
	make that shape. Each grid element will contain a real value between 0 and 1 and the gesture's vectors
	will be rendered, after being scaled and possibly rotated, over this grid. The sum the weights of the
	grid over which the renderer passed will be totaled and then divided by the number of grid elements
	crossed giving a resultant weight of how closely the gesture matches the Shape. This means that gestures
	that make partial matches, for example, a gesture of a line rendered on a Shape of a square, would return
	a near match. To prevent this, the Zone objects describe areas of the Shape that must be crossed before a
	match will be considered, as in the example, putting a Zone on each corner of the square would prevent a
	line being matched. A Zone is made up of an enter and exit radius (how far the gesture's vertex must be
	from the zone to be considered as having entered or exited), an enter and exit angle (the angle the
	gesture's vector must enter or exit from), and an enter and exit arc (same as angle, but gives a range of
	angles off the enter/exit angle). Only if the gesture enters the enter radius, angle and arc, and exits
	the exit radius, angle and arc, or doesn't exit, is the zone considered as having been used. In general,
	a gesture isn't just the 2 dimensional projected movement from the user, it is also the speed or
	acceleration of a movement, plotted against time. To account for the different meanings of gesture, the
	Shape class is inherited as three sub-classes, ShapeMovement, ShapeSpeed, and ShapeAccel. The purpose of
	these classes is to extract the necessary information from the movement, and compare it against the
	Shape, i.e. the ShapeSpeed class extracts the vectors length as the y co-ordinate of the movement, and
	time as the x co-ordinate, and the Shape then renders that to compare the Shape. The ShapeMovement also
	contains a ShapesCollection specifically for ShapeSpeed and ShapeAccel collections, which allow a
	ShapeMovement to have different results based on the suddeness or sharpness of the movement, by having a
	collection of different Speed and Accel Shapes.
	After the Shapes have been loaded, a Recorder object is created and passed on to the Parser[DS]. The
	Parser reads the data from the WiiMote's Connection[JD] and passes it on the Recorder in simple terms.
	The Recorder captures this simple data, and stores it in a way that will be described below, but for now,
	the result of storing this data will be the gesture. The Recorder is sub-classed from an interface
	IParserRecorder, and this is what allows the Parser to talk to the Recorder. The IParserRecorder gives
	the Parser two methods to communicate with the Recorder and these are Control and Record. Control allows
	the Parser to give new state information to the Recorder such as 'Found new IR blob', or 'Connection with
	WiiMote lost' allowing the Recorder to allocate or de-allocate storage as needed. The Record method allows
	the Parser to tell the Recorder the positions of each IR blob individually using an IRID (IR blob
	Identification). For each unique IRID, the Recorder holds a unique Track object, and within each Track,
	for each Record call that the Parser makes, the Recorder stores the information in a Frame object, which
	is stored in the Track. The Frame is just a data structure holding (x, y) co-ordinate of the IR blob, the
	(x, y) vector to the next Frame (this information is only available when a next Frame is added), the size
	of the IR blob, and the time the Frame was created. The time is calculated as the number of milliseconds
	since the Kinephon program began. The Frame also handles adding new Frame objects by linking it to the
	last in a linked-list. The Track is just a container for Frame objects (and from above, is a gesture that
	can be matched against the Shapes) by holding a reference to the first Frame in the list, but also holds
	the IRID, and a helper for accessing a Frame by index number and a count of frames. Due to the fact that
	the Parser will be running asynchronously, so Frame objects will be created at non-deterministic times,
	the Recorder will have two functions for dealing with the Tracks, these are Eject and Erase. Eject will
	be a critical section that blocks the Parser and will create a copy of all the Track and Frame objects
	and store then in a Recording object, which it will then return. Kinephon can work off this static
	Recording while ignoring the fact that the Recorder may be receiving new Frame objects. A Recording is a
	basic Recorder with no Eject or Erase functionality, just a holder for Tracks and their Frames. Erase
	will be another critical section that blocks the Parser and will allow the caller (described below during
	the main loop) to ask for frames to be deleted by a frame index. All Frames before and including this
	frame index will be removed from the Recorder's Track.
	After initialisation, the main loop will begin. The main loop will perform the following steps on the
	Recorder and Analyser. First, it calls the Recorder's Eject to get a copy of the latest gesture's (Tracks
	and Frames) made by each IR blob. Each gesture in turn is passed on to the Shapes object along with a
	ShapeMatches object. The ShapeMatches is a container for keeping a list of shapes that matched and when
	it's constructed, can be given a weight range and total that specifies the minimum weight that a gesture
	must match a Shape and the maximum number of shapes that can be matched (if more shapes match, the list
	is trimmed to the top 'total' amount). As the Shapes object compares the gesture against all the Shape
	objects it contains, for each one that matches, it creates a ShapeMatch object and stores a reference to
	the Shape, along with the weight of the match in it, the ShapeMatch is then added to the ShapeMatches,
	which sorts and trims its current list. When the Shapes compare returns, ShapeMatches will then contain
	this sorted list of Shape objects matched within the given weight range and this list is passed on to the
	Interpreter[SA]. The Interpreter converts the matched shapes into the audio controls and passes them on
	to Audio[ED]. The Audio generates and plays the music based on the audio controls. Once the Interpreter
	has used a matched shape, it returns that information back so that that Shape's Frame index can be marked
	for deletion. The Recorder is then asked to Erase those frames that have been used. If the Interpreter
	does not use the data (it may not a good time to use it musically) it will not be deleted and will be
	reprocessed next time the Recorder is asked to Eject.

Synthesis [Eva]
	The Synthesis consists of two classes: MidiPlayer and Conductor. The former represents a middle layer of
	abstraction between the high-level interface of the Conductor and the (relative) low-level interface of the
	sound generation classes.
	
	MIDI messaging works with 'unsigned char', that is, every setting is encoded as a number.
	To avoid using 'Magic Numbers' and ending up in a confusing lump of meaningless numbers, enumerated types 
	will be used in the Synthesis and in the Sound Generation part of this project. E.g. there will be enumerated 
	types for Chords, Middle Notes, Rhythms, Dynamics and Textures, so these will basically hide the numeric details
	and only use the high-level musical representations. 
	
	MidiPlayer:
		(Formerly this class was called AudioController; it was renamed for naming consistency.)
		This class provides facilities to connect to available Midi ports and to play MIDI audio based on several 
		dedicated channels, such as rhythm, chords and percussion channels. 
		This class provides a middle level of abstraction, so it is kept intentionally technical. It is intended 
		not to obscure the details of the Midi messaging mechanism.  
		
		The class currently has 4 dedicated channels: lead, accompaniment, chords, percussion. Each of them can 
		be controlled individually, thus allowing for different configurations on each. With this approach
		it is possible to play different instruments at once, at different volume levels and with different 
		effects, which results in a more authentic music experience.
		 
		The main tasks of this class are:
		- finding available MIDI output ports, including the ones that are being opened by software synthesisers. 
	 	  If more than one is available, the user is given a choice to pick one, subsequently establishing a connection.
	 	- sending control MIDI messages to the correct channels; there are the following message formats:
	 		* Channel Mode	
	 		* Control Change
	 		* Program Change
	 		* System Exclusive
	 	- sending messages to play notes on the different channels (Note On MIDI messages). 
	 	  Each channel is being controlled by a separate method.
	 	- sending a generic Note On message. Although this method is redundant for controlling the 4 dedicated
	 	  channels, this function is included for completeness. It may be desirable in future to send a message
	 	  to a channel other than the 4 since, theoratically, MIDI supports 16 simultaneous channels.
	 	- sending messages to release playing notes. This functionality is vital, since MIDI does not stop
	 	  playing a note automatically. Whithout releasing notes, one would very quickly get an indistiguishable
	 	  blur of noise.
	 	  Releasing notes on the channels will also have to be done separately, since some notes should be played
	 	  longer than others.
	 	- remember configurations, such as last played notes. This is necessary for some MIDI messages, however the
	 	  user should have to worry about these, so the class does this automatically.
	 	- handling exceptions. These should not be propagated into the upper levels but caught and handles here.
	 	  If initial connection to a MIDI port has failed, feedback in form of a boolean return variable will be
	 	  given. Any following errors, like wrong messages formats, resulting in Exceptions being thrown, will be
	 	  caught, but ingnored. I.e. no backward feedback will be given, since none of these errors are critical
	 	  for the working of the rest of the program.
	 	  
	 	The class can be easily extended if future development (of the sound part or the moevement analysis part of
	 	the project) will require additional (specialized/dedicated) functions.
	 	
	Conductor:
		The conductor is wrapper class for the MidiPlayer providing a high-level musically oriented interface.
		This class will be the interface between the Interpreter and the Sound Generation part of this project.
		It is intended to hide all MIDI related details.
		  
		It works in basically two stages:
		1.) Setting up the parts of the music piece. It will provide methods for specifying rhythms, dynamics,
			textures, chords and other effects, or whether these should be switched off. The methods will then 
			automatically call the correct function for the correct channels in the MidiPlayer class.
			These settings can be carried out any time, since they themselves do not result in any audible feedback. 
		
		2.) Playing audio. This method will actually produce a sound. To satisfy the timing constraints of music,
			this method relies upon being called in regular time intervals. 
	 



Generation [Eva]
	We will use the OpenSource RtMidi classes. They provide basic low-level functionality such as probing for ports
	and opening available ones. An RtError class provides the appropriate exception handling.
	[LINUX]
		Since on-board sound cards do not have built in Midi support we need to use a software synthesizer.
		The choice will most probably be 'fluidsynth' together with an appropriate soundfont file.
		'fluidsynth' is a command-line synthesizer, so it will be necessary to incorporate it into to project
		(at a later stage).
	[Windows] 
		Windows does have an already built-in synthesizer Microsoft GS Wavetable SW Synth, but this will have to be tested
		if it fits our purposes, otherwise another one will have to be found.
		
Sound Recorder [Eva]
	This will need some more reasearch as to how exactly Midi files are stored, i.e. what the header contains, 
	what format the messages are stored in and how it is ended.

####
#	There are several documents on MID file format here http://www.wotsit.org/list.asp?al=M
####

Visualisation
	The visualisation will consist of colour and movement. The points that are being tracked will appear as points on the screen. As the points move a trail like that behind a comet will be produced.
	Depending on the notes being generated a new colour will be appended to the tail as the motion continues.
	Drawing will be done using an OpenGL buffer and a GLUT window.
	Will these points on screen be sprites? Or will we plot them once and blend the out using fullscreen blur/warp effects?

Visual Recorder
	
	
Movement Recorder
	All Recorder animation frames will be passed on to this and saved as raw data as they are received. This will
	 mean that frames from different animations will be interleaved with other animations, but when loading, the frame's
	 blob id will link it back to its animation. Frames will be saved in order and so can be loaded in the same order
	 
Web app
	The web app has been described in more detail in Sumbo_WebService_Analysis.doc

Dictionary:
Blob : Single IR light or reflective IR tape picked up by the Camera as an (X, Y) co-ordinate and radius