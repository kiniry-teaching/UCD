<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Kinephon - Project Design</title>
</head>
<body>
<h1>Kinephon</h1>
	<h2>Revision</h2>
		<table>
			<tr><td>0.1</td><td>ED</td><td>First draft</td></tr>
			<tr><td>0.2</td><td>EB</td><td>Added Recorder and Analyser</td></tr>
			<tr><td>0.2</td><td>ED</td><td>Finished Sound Part</td></tr>
		</table>
	<h2>About</h2>
		<table>
			<tr><td><b>Title:</b></td><td>Kinephon</td><td></td></tr>
			<tr><td><b>Team:</b></td><td>Intuitive Aptitudes</td><td></td></tr>
			<tr><td><b>Members:</b></td><td>Eva Darulova</td><td>05817625</td></tr>
			<tr><td></td><td>Sumbo Ajisafe</td><td>05005451</td></tr>
			<tr><td></td><td>David Swords</td><td>05477689</td></tr>
			<tr><td></td><td>Eliott Bartley</td><td>05806500</td></tr>
			<tr><td></td><td>Jakub Dostal</td><td>05844151</td></tr>
		</table>
	<h2>Installer</h2>
	<p>
	    This installer could come in different flavours, depending on how the project comes along.
	    The two main options are one for Linux and one for Windows.
    </p>
   	<h3>Linux:</h3>
	<p>
		This will be a .deb (Debian) package. Itself can come in two flavours which are binary and
		source. A binary package contains all the executables, man pages etc. and are unpacked using
		the Debian utility dpkg. A source package contains a .dsc file describing the source package itself,
		a orig.tar.gz containing the original source code and a diff.gz describing the Debian specific changes
		to the source. The dpkg-source, packs and unpacks the Debian source archives.
	</p>
	<h3>Windows:</h3>
	<p>
		For this it will be likely that we will produce the MSI files with a Third-party app.
		Though we could also use Visual Studio 8 to produce an installer also.
	</p>

	<h2>Configuration</h2>
	<p>...</p>

    <h2>Recorder and Analyser[EB]</h2>
    <p>
	    During initialisation, Kinephon will call on the ShapeLoader. ShapeLoader will open a file containing all
	    the different gestures and load them in, returning a Shapes collection. A gesture is a physical movement
	    made by the user, recorded by the WiiMote, and stored as an array of vectors in 2 dimensional space (a
	    gesture actually has a more general meaning, but this is described below). A Shapes collection has two
	    functions; hold an array of Shape objects, and compare a single gesture against each shape to find near
	    matches. A Shape is made up a 2 dimensional array of 'weights', an array of Zone objects, and an SID (a
	    Shape id). The 2 dimensional array of 'weights' describe a grid over the most likely areas which a gesture
	    will travel to make that shape. Each grid element will contain a real between 0 and 1 and the gesture's
	    vectors will be rendered (after being scaled and possibly rotated) over this grid. The sum the weights of
	    the grid over which the renderer passes will be totalled and then divided by the number of grid elements
	    crossed giving a resultant weight of how closely the gesture matches the Shape. This means that gestures
	    that make partial matches, for example, a gesture of a line rendered on a Shape of a square, would return
	    a near match. To prevent this, the Zone objects describe areas of the Shape that must be crossed before a
	    match will be considered - as in the example, putting a Zone on each corner of the square would prevent a
	    line being matched. A Zone is made up of an enter and exit radius (how far the gesture's vertex must be
	    from the zone to be considered as having entered or exited), an enter and exit angle (the angle the
	    gesture's vector must enter or exit from), and an enter and exit arc (same as angle, but gives a range of
	    angles off the enter/exit angle). Only if the gesture enters the enter radius, angle and arc, and exits
	    the exit radius, angle and arc, or doesn't exit, is the zone considered as having been used. In general,
	    a gesture isn't just the 2 dimensional projected movement from the user, it is also the speed or
	    acceleration of a movement, plotted against time. The gesture is generalised further in the
	    Interpreter[SA] but this is not described here. To account for the different meanings of gesture, the
	    Shape class is inherited as three sub-classes, ShapeMovement, ShapeSpeed, and ShapeAccel. The purpose of
	    these classes is to extract the necessary information from the movement, and compare it against the
	    Shape, i.e. the ShapeSpeed class extracts the vector's length as the y co-ordinate of the movement, and
	    time as the x co-ordinate, and the Shape then renders that to compare the Shape. The ShapeMovement also
	    contains a Shapes collection specifically for ShapeSpeed and ShapeAccel collections, which allow a
	    ShapeMovement to have different results based on the suddeness or sharpness of the movement, by having a
	    collection of different Speed and Accel Shapes. Different shapes (whether they are movement, speed, or
	    acceleration, are ultimately identified by the SID.
	</p>
	<p>
	    After the shapes have been loaded, a Recorder object is created and passed on to the Parser[DS]. The
	    Parser reads the data from the WiiMote's Connection[JD] and passes it on the Recorder in simple terms.
	    The Recorder captures this simple data, and stores it in a way that will be described below, but for now,
	    the result of storing this data will be the gesture. The Recorder is sub-classed from an interface
	    IParserRecorder, and this is what allows the Parser to talk to the Recorder. The IParserRecorder gives
	    the Parser two methods to communicate with the Recorder and these are Control and Record. Control allows
	    the Parser to give new state information to the Recorder such as 'Found new IR blob', or 'Connection with
	    WiiMote lost' allowing the Recorder to allocate or de-allocate storage as needed. The Record method allows
	    the Parser to tell the Recorder the positions of each IR blob individually using an IRID (IR blob
	    Identification). For each unique IRID, the Recorder holds a unique Track object, and within each Track
	    (for each Record call that the Parser makes) the Recorder stores the information in a Frame object, which
	    is stored in the Track. The Frame is just a data structure holding an (x, y) co-ordinate of the IR blob, the
	    (x, y) vector to the next Frame (this information is only available when a next Frame is added), the size
	    of the IR blob, and the time the Frame was created. The time is calculated as the number of milliseconds
	    since the Kinephon program began. The Frame also handles adding new Frame objects by linking it to the
	    last in a linked-list. The Track is just a container for Frame objects (or abstractly, is the gesture that
	    can be matched against the Shapes) by holding a reference to the first Frame in the list, but also holds
	    the IRID, a helper for accessing a Frame by index number, and a count of frames. Due to the fact that
	    the Parser will be running asynchronously (i.e. Frame objects will be created at non-deterministic times)
	    the Recorder will have two functions for dealing with the Tracks, these are Eject and Erase. Eject will
	    be a critical section that blocks the Parser and will create a copy of all the Track and Frame objects
	    and store then in a Recording object, which it will then return. Kinephon can work off this static
	    Recording while ignoring the fact that the Recorder may be receiving new Frame objects. A Recording is a
	    basic Recorder with no Eject or Erase functionality, just a holder for Tracks and their Frames. Erase
	    will be another critical section that blocks the Parser and will allow the caller (described below during
	    the main loop) to ask for frames to be deleted by a frame index. All Frames before and including this
	    frame index will be removed from the Recorder's Track.
	</p>
	<p>
	    After initialisation, the main loop will begin. The main loop will perform the following steps on the
	    Recorder and Analyser. First, it calls the Recorder's Eject to get a copy of the latest gestures (Tracks
	    and Frames) made by each IR blob. Then each gesture in turn is passed on to the Shapes object along with
	    a ShapeMatches object. The ShapeMatches is a container for keeping a list of Shape objects that matched.
	    When it is constructed, the ShapeMatches can be given a 'weight range' and 'total' that specifies the
	    minimum weight that a gesture must match a Shape by, and the maximum number of shapes that can be matched
	    (if more shapes match, the list is trimmed to the best matched 'total' amount). As the Shapes object compares
	    the gesture against all the Shape objects it contains, for each one that matches, it creates a ShapeMatch
	    object and stores a reference to the Shape, along with the weight of the match, in this. The ShapeMatch is
	    then added to the ShapeMatches, which sorts and trims its current list. When the Shapes's compare returns,
	    ShapeMatches will then contain this sorted list of Shape objects matched within the given weight range and
	    this list is passed on to the Interpreter[SA]. The Interpreter converts the matched shapes into the audio
	    controls and passes them on to Audio[ED]. The Audio generates and plays the music based on the audio
	    controls. If no Shape matches were made, the gesture is still passed on to Interpreter so that the
	    Interpreter can try to calculate the flow of the gesture, rather than a specific shape. Once the Interpreter
	    has used a gesture, either specifically or generally, it returns that information back so that that Frame
	    index can be marked for deletion. The Recorder is then asked to Erase those frames that have been used. If
	    the Interpreter does not use the gesture (it may not a good time to use it musically) it will not be
	    deleted and will be reprocessed next time when the Recorder is asked to Eject.
    </p>

 	<h2>Sound Synthesis and Generation [ED]</h2>
    <p>
    	The Synthesis consists of two classes: MidiPlayer and Conductor. The former represents a middle layer of
		abstraction between the high-level interface of the Conductor and the (relative) low-level interface of the
		sound generation classes.
	
		MIDI messaging works with 'unsigned char', that is, every setting is encoded as a number.
		To avoid using 'Magic Numbers' and ending up in a confusing lump of meaningless numbers, enumerated types 
		will be used in the Synthesis and in the Sound Generation part of this project. E.g. there will be enumerated 
		types for Chords, Middle Notes, Rhythms, Dynamics and Textures, so these will basically hide the numeric details
		and only use the high-level musical representations. 
	
	<h3>MidiPlayer:</h3>
		(Formerly this class was called AudioController; it was renamed for naming consistency.)
		This class provides facilities to connect to available Midi ports and to play MIDI audio based on several 
		dedicated channels, such as rhythm, chords and percussion channels. 
		This class provides a middle level of abstraction, so it is kept intentionally technical. It is intended 
		not to obscure the details of the Midi messaging mechanism.  
		
		The class currently has 4 dedicated channels: lead, accompaniment, chords, percussion. Each of them can 
		be controlled individually, thus allowing for different configurations on each. With this approach
		it is possible to play different instruments at once, at different volume levels and with different 
		effects, which results in a more authentic music experience.
		 
		
		<ul>
		The main tasks of this class are:
		<li> finding available MIDI output ports, including the ones that were opened by software synthesisers. 
	 	  If more than one is available, the user is given a choice to pick one, subsequently establishing a connection.
	 	<li> sending control MIDI messages to the correct channels; there are the following message formats:
	 		<table>
	 		<tr><td>* Channel Mode</td><td>
	 		<tr><td>* Control Change</td><td>
	 		<tr><td>* Program Change</td><td>
	 		<tr><td>* System Exclusive</td><td>
	 		</table>
	 	<li> sending messages to play notes on the different channels (Note On MIDI messages). 
	 	  Each channel is being controlled by a separate method.
	 	<li> sending a generic Note On message. Although this method is redundant for controlling the 4 dedicated
	 	  channels, this function is included for completeness. It may be desirable in future to send a message
	 	  to a channel other than our four, since, theoratically MIDI supports 16 simultaneous channels.
	 	<li> sending messages to release playing notes. This functionality is vital, since MIDI does not stop
	 	  playing a note automatically. Whithout releasing notes, one would very quickly get an indistiguishable
	 	  blur of noise.
	 	  Releasing notes on the channels will also have to be done separately, since some notes should be played
	 	  longer than others.
	 	<li> remember configurations, such as last played notes. This is necessary for some MIDI messages, however the
	 	  user should have to worry about these, so the class does this automatically.
	 	<li> handling exceptions. These should not be propagated into the upper levels but caught and handled here.
	 	  If initial connection to a MIDI port has failed, feedback in form of a boolean return variable will be
	 	  given. Any following errors, like wrong messages formats, resulting in exceptions being thrown, will be
	 	  caught, but ingnored. I.e. no backward feedback will be given, since none of these errors are critical
	 	  for the working of the rest of the program.
	 	</ul>  
	 	The class can be easily extended if future development (of the sound part or the moevement analysis part of
	 	the project) will require additional (specialized/dedicated) functions.
	 	
	<h3>Conductor:</h3>
		The Conductor is wrapper class for the MidiPlayer providing a high-level musically oriented interface.
		This class will be the interface between the Interpreter and the Sound Generation part of this project.
		It is intended to hide all MIDI related details and only work with musical terminology. This will require
		some basic understanding of how music works, however this class is intended to help the user by automating as
		much as possible while still allowing for enough artistic freedom.
		</p>
		It works basically in two stages:
		<table>
		1.) Setting up the parts of the music piece. This is more or less equivalent to composing a music piece.
			Methods are provided for specifying rhythms, dynamics, textures, chords and other effects as and when required. 
			These will then call the correct function for the correct channels in the MidiPlayer class.
			These settings can be carried out at any time, since they themselves do not result in any audible feedback,
			they are only setting up attributes of the composition.
		</p>
		2.) Playing audio. This method will actually produce a sound. To satisfy the timing constraints of music,
			this method relies upon being called in regular time intervals, e.g. every quarter of a note, or every eigth of
			the note. How much time actually passes between the calls is irrelevant for the Conductor, since music also
			only relies on the relative intervals. 
	 	</table>

		This class again can be arbitrarily extended as development of the project progresses. Some of the movement analysis
		may require a specific effect on the music, so the structure of the class has been set up in a way that will easily
		cater for any future demand. This is also one of the strength of the MIDI technology.
    </p>
    <h3>Channel:</h3>
    <p>
    	The Channel class is a Container class representing a single MIDI channel. It encapsulates the low-level details,
    	i.e. the bit manipulations, of the MIDI messaging mechanism, as well as saving the current state of this channel.
    	This information is important for some MIDI actions, such as releasing notes or determining matching chords or accompaniment.
    	This class is supposed to be low-level, so it does not deal with exceptions, it only propagates them.
    	
    <h3>RtMidi</h3>
    <p>
    	This project makes use of the OpenSource RtMidi classes (©2003-2007 Gary P. Scavone, McGill University)
    	RtMidiIn and RtMidiOut.
    	They provide basic low-level functionality such as probing for ports and opening available ones, providing
    	code for these services on the UNIX, Windows and Mac OS platforms.
    	An RtError class provides the appropriate exception handling, thus providing a solid basis.
    </p>
    
    <h3>MidiRecorder:</h3>
    <p>
    	This class will write the audio output produced into a MIDI file.
    	It will not be accessible directly from the user, but will be managed from the MidiPlayer class, since this
    	class will also have all the information that will need to be recorded.
    	MIDI files consist of a MThd header chunk that identifies the type of file as a MIDI file and specifies
    	its format.
    	MIDI files are build up of separate tracks which are being written to the file successively. This means that
    	the MidiRecorder class will need buffer for each separate track, in out case we will probably have one track for each
    	one of the channels used by the MidiPlayer class. Only after the music piece is completed the data from these buffer
    	will be written to the file.
    	The actual part of writing to file may turn out to be platform dependent. The first aim will be to enable this feature
    	for the LINUX system, and only after that for the Windows platform as well. 
    </p>
    
   <b>Note:</b> Normal PC computers do not have hardware MIDI support. Hence we are forced to use Software Synthesizer fro this project.
   For the LINUX platform the development is currently using the command line synthesizer 'fluidsynth' with the soundfont PC51f.sf2.
   These settings may change, however the choice of synthesizers does not influence the music piece as such, only the particular
   quality of the sound. That is, the acoustic grand piano will sound slightly differently on each synthesizer.
   As for the Windows platform, it has a MIDI synthesizer build in; the SW-Synthesizer. Currently, it seems to be suitable for this project.
    
 
</body>
</html>